---
title: "ridgereg"
author: "Niclas Lovsj√∂, Maxime Bonneau"
date: "11 oktober 2015"
output: pdf_document
---

We have built the function ridgereg, which makes ridge regression out of a dataset by being fed a formula and some data. We can also define a lambda, which the default is set to 0. This is done like this,

```{r,message=FALSE}
library(lab4bis)
data(iris)
model1<-ridgereg(Sepal.Length~Sepal.Width+Petal.Length,iris)
model1$coefficients
model2<-ridgereg(Sepal.Length~Sepal.Width+Petal.Length,iris, lambda=2)
model2$coefficients
```
  
Predictions are done by,
  
```{r}
newdata<-data.frame(c(1,2,3),c(4,3,2))
predict(model2,newdata)
```

<<<<<<< HEAD
Now we will comment on the caret-package part. We have decided to use our own API data from lab5. In there we have data of the election of year 2014 in Sweden. We will try to build a model explaining the size of a city, in terms of people allowed to vote, by the distribution of votes the parties in that particular city has. For example, say the distribution of the great little city Filipstad has 40%(S),30%(SD),20%(V),5%(M). Then can we find a good prediction of the size of this city? The features will then be the party percentages.  
  
First we divide the data into a training and a test set:
```{r}
#extract the data we need and partition:
library(lattice)
library(caret)
divide_data<-function(theData){
  data <- data.frame(theData[,unlist(lapply(colnames(theData),function(y) substr(y,start=nchar(y)-2,stop=nchar(y))))=="tal"])
  data<-data.frame(data,theData$Rostb)
  colnames(data)<-c(colnames(theData)[unlist(lapply(colnames(theData),function(y) substr(y,start=nchar(y)-2,stop=nchar(y))))=="tal"],"Rostb")
  set.seed(12345)
  in_train <- createDataPartition(y = data$Rostb,p=0.75,list = FALSE)
  train <- theData[in_train,]
  test<-theData[-in_train,]
  out<-list(train=train,test=test)
  #how to actually get the coefficients???
  return(out)
}
data<-divide_data(theData)
```
  
This is how the data will look like:
```{r}
data$train[1:2,]
```
  
Then we fit a linear model using the caret package, and also a linear model using foreward-selection.  
  
```{r,eval=FALSE}
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 10)
set.seed(12345)
model1<-train(data$train$Rostb~.,data=data$train,method="lm",trControl=fitControl)
set.seed(12345)
model2<-train(data$train$Rostb~.,data=data$train,method="leapForward",trControl=fitControl)
```
  
The performance on the training set is that we get an root mse of 49129 for all of the features, which seem very high. Doing the forward-selection we get that the model with 4 features performs the best having a root mse of 1208, which is a clear improvement from all the features. The full model also only explains 0.22 while the 4 feature model explains .99. Perhaps this is even "too good", and one could suspect that this way of doing this analysis isnt that well suited. 


## "dplyr" package

Thanks to "dplyr" package, we were able to calculate the mean delays from the datasets from nycflights13 package.
This is a package in which we can find delays for several flights from New York City airports to some other American airports. We decided to use 'ggmap' to get a good visualization of these delays.

```{r}
visualize_airport_delays()
```

